{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "torch.manual_seed(1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa0e8e4b6b0>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "ngrams = [\n",
    "    (\n",
    "        [test_sentence[i - j - 1] for j in range(CONTEXT_SIZE)],\n",
    "        test_sentence[i]\n",
    "    )\n",
    "    for i in range(CONTEXT_SIZE, len(test_sentence))\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "ngrams"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(['forty', 'When'], 'winters'),\n",
       " (['winters', 'forty'], 'shall'),\n",
       " (['shall', 'winters'], 'besiege'),\n",
       " (['besiege', 'shall'], 'thy'),\n",
       " (['thy', 'besiege'], 'brow,'),\n",
       " (['brow,', 'thy'], 'And'),\n",
       " (['And', 'brow,'], 'dig'),\n",
       " (['dig', 'And'], 'deep'),\n",
       " (['deep', 'dig'], 'trenches'),\n",
       " (['trenches', 'deep'], 'in'),\n",
       " (['in', 'trenches'], 'thy'),\n",
       " (['thy', 'in'], \"beauty's\"),\n",
       " ([\"beauty's\", 'thy'], 'field,'),\n",
       " (['field,', \"beauty's\"], 'Thy'),\n",
       " (['Thy', 'field,'], \"youth's\"),\n",
       " ([\"youth's\", 'Thy'], 'proud'),\n",
       " (['proud', \"youth's\"], 'livery'),\n",
       " (['livery', 'proud'], 'so'),\n",
       " (['so', 'livery'], 'gazed'),\n",
       " (['gazed', 'so'], 'on'),\n",
       " (['on', 'gazed'], 'now,'),\n",
       " (['now,', 'on'], 'Will'),\n",
       " (['Will', 'now,'], 'be'),\n",
       " (['be', 'Will'], 'a'),\n",
       " (['a', 'be'], \"totter'd\"),\n",
       " ([\"totter'd\", 'a'], 'weed'),\n",
       " (['weed', \"totter'd\"], 'of'),\n",
       " (['of', 'weed'], 'small'),\n",
       " (['small', 'of'], 'worth'),\n",
       " (['worth', 'small'], 'held:'),\n",
       " (['held:', 'worth'], 'Then'),\n",
       " (['Then', 'held:'], 'being'),\n",
       " (['being', 'Then'], 'asked,'),\n",
       " (['asked,', 'being'], 'where'),\n",
       " (['where', 'asked,'], 'all'),\n",
       " (['all', 'where'], 'thy'),\n",
       " (['thy', 'all'], 'beauty'),\n",
       " (['beauty', 'thy'], 'lies,'),\n",
       " (['lies,', 'beauty'], 'Where'),\n",
       " (['Where', 'lies,'], 'all'),\n",
       " (['all', 'Where'], 'the'),\n",
       " (['the', 'all'], 'treasure'),\n",
       " (['treasure', 'the'], 'of'),\n",
       " (['of', 'treasure'], 'thy'),\n",
       " (['thy', 'of'], 'lusty'),\n",
       " (['lusty', 'thy'], 'days;'),\n",
       " (['days;', 'lusty'], 'To'),\n",
       " (['To', 'days;'], 'say,'),\n",
       " (['say,', 'To'], 'within'),\n",
       " (['within', 'say,'], 'thine'),\n",
       " (['thine', 'within'], 'own'),\n",
       " (['own', 'thine'], 'deep'),\n",
       " (['deep', 'own'], 'sunken'),\n",
       " (['sunken', 'deep'], 'eyes,'),\n",
       " (['eyes,', 'sunken'], 'Were'),\n",
       " (['Were', 'eyes,'], 'an'),\n",
       " (['an', 'Were'], 'all-eating'),\n",
       " (['all-eating', 'an'], 'shame,'),\n",
       " (['shame,', 'all-eating'], 'and'),\n",
       " (['and', 'shame,'], 'thriftless'),\n",
       " (['thriftless', 'and'], 'praise.'),\n",
       " (['praise.', 'thriftless'], 'How'),\n",
       " (['How', 'praise.'], 'much'),\n",
       " (['much', 'How'], 'more'),\n",
       " (['more', 'much'], 'praise'),\n",
       " (['praise', 'more'], \"deserv'd\"),\n",
       " ([\"deserv'd\", 'praise'], 'thy'),\n",
       " (['thy', \"deserv'd\"], \"beauty's\"),\n",
       " ([\"beauty's\", 'thy'], 'use,'),\n",
       " (['use,', \"beauty's\"], 'If'),\n",
       " (['If', 'use,'], 'thou'),\n",
       " (['thou', 'If'], 'couldst'),\n",
       " (['couldst', 'thou'], 'answer'),\n",
       " (['answer', 'couldst'], \"'This\"),\n",
       " ([\"'This\", 'answer'], 'fair'),\n",
       " (['fair', \"'This\"], 'child'),\n",
       " (['child', 'fair'], 'of'),\n",
       " (['of', 'child'], 'mine'),\n",
       " (['mine', 'of'], 'Shall'),\n",
       " (['Shall', 'mine'], 'sum'),\n",
       " (['sum', 'Shall'], 'my'),\n",
       " (['my', 'sum'], 'count,'),\n",
       " (['count,', 'my'], 'and'),\n",
       " (['and', 'count,'], 'make'),\n",
       " (['make', 'and'], 'my'),\n",
       " (['my', 'make'], 'old'),\n",
       " (['old', 'my'], \"excuse,'\"),\n",
       " ([\"excuse,'\", 'old'], 'Proving'),\n",
       " (['Proving', \"excuse,'\"], 'his'),\n",
       " (['his', 'Proving'], 'beauty'),\n",
       " (['beauty', 'his'], 'by'),\n",
       " (['by', 'beauty'], 'succession'),\n",
       " (['succession', 'by'], 'thine!'),\n",
       " (['thine!', 'succession'], 'This'),\n",
       " (['This', 'thine!'], 'were'),\n",
       " (['were', 'This'], 'to'),\n",
       " (['to', 'were'], 'be'),\n",
       " (['be', 'to'], 'new'),\n",
       " (['new', 'be'], 'made'),\n",
       " (['made', 'new'], 'when'),\n",
       " (['when', 'made'], 'thou'),\n",
       " (['thou', 'when'], 'art'),\n",
       " (['art', 'thou'], 'old,'),\n",
       " (['old,', 'art'], 'And'),\n",
       " (['And', 'old,'], 'see'),\n",
       " (['see', 'And'], 'thy'),\n",
       " (['thy', 'see'], 'blood'),\n",
       " (['blood', 'thy'], 'warm'),\n",
       " (['warm', 'blood'], 'when'),\n",
       " (['when', 'warm'], 'thou'),\n",
       " (['thou', 'when'], \"feel'st\"),\n",
       " ([\"feel'st\", 'thou'], 'it'),\n",
       " (['it', \"feel'st\"], 'cold.')]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "\n",
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "model(torch.tensor([1,2]))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-4.3716, -4.6812, -4.7513, -4.2395, -4.5980, -4.4615, -4.3640, -4.5541,\n",
       "         -4.4755, -4.3974, -4.5871, -4.2178, -4.4652, -4.6792, -4.6893, -4.5764,\n",
       "         -4.3727, -4.3335, -4.7382, -4.5967, -4.4715, -4.6345, -4.6357, -4.9677,\n",
       "         -4.9105, -5.2268, -4.3867, -4.5053, -5.1275, -4.3163, -4.7395, -4.7795,\n",
       "         -4.6904, -4.7321, -4.7658, -4.8316, -4.4296, -4.8624, -4.6846, -4.7190,\n",
       "         -4.7234, -4.3837, -4.8154, -4.8126, -4.6532, -4.2744, -4.7025, -4.6811,\n",
       "         -4.2640, -4.7771, -4.4732, -4.6610, -4.4694, -4.3045, -4.8156, -4.9493,\n",
       "         -4.4761, -4.8769, -4.8578, -4.6262, -4.7998, -4.6201, -4.8968, -4.3798,\n",
       "         -4.6686, -4.6119, -4.7188, -4.4074, -4.0597, -4.1301, -4.1384, -4.3515,\n",
       "         -4.5970, -4.1615, -4.7674, -4.3190, -4.8796, -4.4806, -4.7584, -4.3875,\n",
       "         -4.2433, -4.8221, -4.7873, -5.1393, -4.6367, -4.9584, -4.7859, -4.7477,\n",
       "         -4.4687, -4.5424, -4.3140, -4.3382, -4.7881, -4.8354, -4.5693, -4.8900,\n",
       "         -4.3149]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "log_probs = model(torch.tensor([1,2]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "loss = loss_function(log_probs, torch.tensor([3], dtype=torch.long))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "loss2 = loss_function(log_probs, torch.tensor([68], dtype=torch.long))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "loss"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(4.2395, grad_fn=<NllLossBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "loss2"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(4.0597, grad_fn=<NllLossBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "torch.argmax(log_probs)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(68)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "log_probs[0,3]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(-4.2395, grad_fn=<SelectBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "log_probs[0,68]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(-4.0597, grad_fn=<SelectBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "loss.backward()\n",
    "optimizer.step()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "interpreter": {
   "hash": "53bc26c3fdbec4bd6ebd68893b07f54171f76704c024cc725a4463474c0c570f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}